---
title: CPU Backend
description: SIMD-optimized CPU implementation as universal fallback
---

# CPU Backend

The CPU backend provides a portable, SIMD-optimized implementation that works on any platform without GPU requirements.

## Overview

The CPU backend serves as:
- Universal fallback when GPU is unavailable
- Development and testing environment
- Reference implementation for correctness verification
- Production option for small-scale deployments

## Features

- **AVX-512** acceleration on modern Intel/AMD CPUs
- **AVX2** fallback for broader compatibility
- **NEON** optimization for ARM64 (Apple Silicon, AWS Graviton)
- **Scalar fallback** for any C99-compatible platform

## Enabling CPU Backend

### CMake Configuration

```bash
cmake -B build \
  -DLATTICE_BACKEND_CPU=ON \
  -DLATTICE_BACKEND_METAL=OFF \
  -DLATTICE_BACKEND_CUDA=OFF
cmake --build build
```

### Runtime Selection

```cpp
#include <lux/lattice/lattice.h>

// Force CPU backend (ignores available GPUs)
lattice_set_backend(LATTICE_BACKEND_CPU);

// Check SIMD level
const char* simd = lattice_cpu_simd_level();
printf("SIMD: %s\n", simd);  // "AVX-512", "AVX2", "NEON", or "scalar"
```

### Go Backend Selection

```go
import "github.com/luxfi/lattice"

// Force CPU backend
lattice.SetBackend(lattice.BackendCPU)

// Or use auto-detection (selects GPU if available)
lattice.SetBackend(lattice.BackendAuto)
```

## Performance Characteristics

### NTT Performance by CPU

| CPU | SIMD | n=4096 | n=16384 | Batch 1000 |
|-----|------|--------|---------|------------|
| Apple M3 | NEON | 45μs | 220μs | 42ms |
| Intel i9-13900K | AVX-512 | 35μs | 180μs | 33ms |
| AMD Ryzen 9 7950X | AVX-512 | 38μs | 190μs | 35ms |
| AWS Graviton3 | NEON | 55μs | 280μs | 52ms |
| Intel Xeon 8380 | AVX-512 | 30μs | 150μs | 28ms |

### CPU vs GPU Comparison (n=4096)

| Platform | Time | Relative |
|----------|------|----------|
| CPU (i9-13900K, AVX-512) | 35μs | 1.0x |
| Metal (M3 Max) | 9μs | 3.9x faster |
| CUDA (RTX 4090) | 5μs | 7.0x faster |
| CUDA (H100) | 3μs | 11.7x faster |

CPU is 4-12x slower than GPU, but still fast enough for many applications.

## SIMD Implementations

### AVX-512 NTT Butterfly

```cpp
// 8-way parallel butterfly using AVX-512
void ntt_butterfly_avx512(uint64_t* a, uint64_t* b, const uint64_t* w,
                          uint64_t q, uint64_t q_inv, size_t count) {
    __m512i vq = _mm512_set1_epi64(q);
    __m512i vq_inv = _mm512_set1_epi64(q_inv);

    for (size_t i = 0; i < count; i += 8) {
        __m512i va = _mm512_loadu_epi64(&a[i]);
        __m512i vb = _mm512_loadu_epi64(&b[i]);
        __m512i vw = _mm512_loadu_epi64(&w[i]);

        // Montgomery multiplication: t = b * w mod q
        __m512i vt = mont_mul_avx512(vb, vw, vq, vq_inv);

        // Butterfly
        __m512i sum = add_mod_avx512(va, vt, vq);
        __m512i diff = sub_mod_avx512(va, vt, vq);

        _mm512_storeu_epi64(&a[i], sum);
        _mm512_storeu_epi64(&b[i], diff);
    }
}
```

### ARM NEON NTT Butterfly

```cpp
// 2-way parallel butterfly using NEON
void ntt_butterfly_neon(uint64_t* a, uint64_t* b, const uint64_t* w,
                        uint64_t q, uint64_t q_inv, size_t count) {
    uint64x2_t vq = vdupq_n_u64(q);

    for (size_t i = 0; i < count; i += 2) {
        uint64x2_t va = vld1q_u64(&a[i]);
        uint64x2_t vb = vld1q_u64(&b[i]);
        uint64x2_t vw = vld1q_u64(&w[i]);

        // Montgomery multiplication
        uint64x2_t vt = mont_mul_neon(vb, vw, q, q_inv);

        // Butterfly
        uint64x2_t sum = add_mod_neon(va, vt, vq);
        uint64x2_t diff = sub_mod_neon(va, vt, vq);

        vst1q_u64(&a[i], sum);
        vst1q_u64(&b[i], diff);
    }
}
```

## Multi-Threading

### OpenMP Parallelization

```cpp
// Enable multi-threaded NTT (for batch operations)
lattice_set_num_threads(8);  // Use 8 threads

// Batch NTT uses OpenMP automatically
lattice_ntt_batch_forward(ctx, polys, 1000);

// Check thread count
int threads = lattice_get_num_threads();
```

### Thread Pool Configuration

```cpp
// Create custom thread pool
LatticeThreadPool* pool = lattice_threadpool_create(16);

// Use specific pool for operations
lattice_set_threadpool(ctx, pool);

// Operations now use custom pool
lattice_ntt_batch_forward(ctx, polys, 1000);

lattice_threadpool_destroy(pool);
```

### Go Concurrency

```go
// Go bindings automatically use GOMAXPROCS goroutines
// for batch operations

// Override thread count for C code
lattice.SetNumThreads(runtime.NumCPU())

// Batch operations parallelize automatically
ring.BatchNTT(polys)  // Uses all cores
```

## Memory Alignment

### Aligned Allocations

```cpp
// CPU backend benefits from aligned memory
// lattice_poly_alloc() returns 64-byte aligned memory

uint64_t* poly = lattice_poly_alloc(ctx);
// poly is 64-byte aligned for AVX-512

// Manual aligned allocation
uint64_t* aligned_poly;
posix_memalign((void**)&aligned_poly, 64, n * sizeof(uint64_t));
```

### Cache Optimization

```cpp
// Prefetch for better cache utilization
for (size_t i = 0; i < n; i += 8) {
    __builtin_prefetch(&poly[i + 64], 0, 3);  // Prefetch read
    // Process poly[i:i+8]
}
```

## Debugging and Development

### Reference Implementation

```cpp
// Enable reference (non-SIMD) implementation for debugging
lattice_cpu_set_simd_level(LATTICE_SIMD_SCALAR);

// Operations now use simple loops
lattice_ntt_forward(ctx, poly);

// Restore auto-detection
lattice_cpu_set_simd_level(LATTICE_SIMD_AUTO);
```

### Correctness Verification

```cpp
// Compare GPU result with CPU reference
uint64_t* gpu_result = lattice_poly_alloc(ctx_gpu);
uint64_t* cpu_result = lattice_poly_alloc(ctx_cpu);

// Copy same input
lattice_poly_copy(gpu_result, input, n);
lattice_poly_copy(cpu_result, input, n);

// Compute on different backends
lattice_set_backend(LATTICE_BACKEND_CUDA);
lattice_ntt_forward(ctx_gpu, gpu_result);

lattice_set_backend(LATTICE_BACKEND_CPU);
lattice_ntt_forward(ctx_cpu, cpu_result);

// Verify match
int match = lattice_poly_compare(gpu_result, cpu_result, n);
assert(match == 0);
```

## Platform-Specific Notes

### macOS (Apple Silicon)

```cpp
// On macOS, CPU backend uses NEON automatically
// Metal is faster but CPU works without GPU access

if (lattice_metal_available()) {
    lattice_set_backend(LATTICE_BACKEND_METAL);
} else {
    lattice_set_backend(LATTICE_BACKEND_CPU);
    printf("Using CPU with NEON\n");
}
```

### Linux

```cpp
// Check for AVX-512 support
if (lattice_cpu_has_avx512()) {
    printf("AVX-512 enabled\n");
} else if (lattice_cpu_has_avx2()) {
    printf("AVX2 enabled\n");
}
```

### Windows

```cpp
// MSVC uses different intrinsics headers
// The API remains the same
#include <lux/lattice/lattice.h>
// No platform-specific code needed
```

## Optimization Tips

### 1. Use Batch Operations

```cpp
// Single NTT has per-call overhead
// Batch operations amortize overhead

// Good: Process many at once
lattice_ntt_batch_forward(ctx, polys, 1000);

// Less efficient: One at a time
for (int i = 0; i < 1000; i++) {
    lattice_ntt_forward(ctx, polys[i]);
}
```

### 2. Enable Correct SIMD Level

```bash
# Compile with appropriate flags
cmake -DCMAKE_CXX_FLAGS="-march=native" ...

# Or specific instruction set
cmake -DCMAKE_CXX_FLAGS="-mavx512f -mavx512dq" ...
```

### 3. Pin Threads to Cores

```cpp
// For consistent performance, pin to cores
#pragma omp parallel
{
    int core = omp_get_thread_num();
    cpu_set_t cpuset;
    CPU_ZERO(&cpuset);
    CPU_SET(core, &cpuset);
    pthread_setaffinity_np(pthread_self(), sizeof(cpuset), &cpuset);
}
```

## When to Use CPU Backend

**Use CPU when:**
- No GPU available
- GPU is busy with other workloads
- Processing small batches (GPU launch overhead dominates)
- Maximum portability required
- Debugging and testing
- Low power consumption needed

**Use GPU when:**
- Processing large batches (100+ polynomials)
- Real-time performance required
- GPU memory available
- Power consumption is not a constraint

## Benchmarking

### Built-in Benchmarks

```cpp
// Run built-in CPU benchmark
LatticeNTTContext* ctx;
lattice_ntt_create(&ctx, 4096, 0x7ffe0001ULL);

LatticeBenchResult result;
lattice_benchmark_ntt(ctx, 1000, &result);

printf("Mean: %.2f μs\n", result.mean_us);
printf("Std: %.2f μs\n", result.std_us);
printf("Min: %.2f μs\n", result.min_us);
printf("Max: %.2f μs\n", result.max_us);
```

### Go Benchmarking

```go
import "testing"

func BenchmarkNTT4096(b *testing.B) {
    ring, _ := lattice.NewRing(4096, []uint64{0x7ffe0001})
    poly := ring.NewPoly()
    ring.SampleUniform(poly)

    b.ResetTimer()
    for i := 0; i < b.N; i++ {
        ring.NTT(poly)
        ring.InverseNTT(poly)
    }
}
```

## Related

- [Metal Backend](/docs/backends/metal) - Apple GPU support
- [CUDA Backend](/docs/backends/cuda) - NVIDIA GPU support
- [NTT Operations](/docs/concepts/ntt) - Algorithm details
